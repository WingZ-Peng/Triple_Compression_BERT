{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and Verify the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncommand following to install \n",
    "\n",
    "# ! pip install transformers\n",
    "# ! pip install datasets\n",
    "# ! pip install torch==2.0.1 torchvision==0.15.2\n",
    "# ! pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing (DistilBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libs\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.quantization as quantization\n",
    "import torch.ao.nn.quantized as nnq\n",
    "import transformers\n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForSequenceClassification,\n",
    "                          TrainingArguments,\n",
    "                          Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'mnli'\n",
    "# task = 'mnli-mm'\n",
    "\n",
    "dataset = load_dataset(\"glue\", task)\n",
    "metric = load_metric(\"glue\", task)\n",
    "# before feed texts to model, need to prepocessing data, it can be done by Transformer Tokenizer\n",
    "batch_size = 16\n",
    "\n",
    "# model_checkpoint = \"bert-base-uncased\"\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "task_to_keys = {\"mnli\": (\"premise\", \"hypothesis\")}\n",
    "sentence1_key, sentence2_key = task_to_keys[task]\n",
    "\n",
    "def preprocess_function(samples):\n",
    "  return tokenizer(samples[sentence1_key], samples[sentence2_key], truncation=True)\n",
    "\n",
    "# use one single command to preprocess train, validation and test data\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# MNLI has 3 labels\n",
    "num_labels = 3\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "metric_name = 'accuracy'\n",
    "model_name = model_checkpoint.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    f\"{model_name}_output\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_key = \"validation_mismatched\" if task == \"mnli-mm\" else \"validation_matched\"\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output two models size\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a evaluation function\n",
    "def evaluate(model, encoded_dataset, mnli_dataset, test_dataset):\n",
    "  matched = 0\n",
    "  N = len(encoded_dataset)\n",
    "  print(f'Total matched samples: {N}')\n",
    "\n",
    "  '''\n",
    "  corresponding encoded number\n",
    "  netural => 1\n",
    "  contradiction => 2\n",
    "  entailment => 0\n",
    "  '''\n",
    "  for i, batches in enumerate(encoded_dataset):\n",
    "    premise = batches['premise']\n",
    "    hypothesis = batches['hypothesis']\n",
    "    idx = batches['idx']\n",
    "    label = mnli_dataset[idx]['label']\n",
    "    # input to model and predict the label\n",
    "    encode_input = tokenizer(premise, hypothesis, return_tensors='pt')\n",
    "    output = model(**encode_input)\n",
    "    # need Tensor.cpu() to copy the tensor to host memory first\n",
    "    pred = np.argmax(output.logits.detach().cpu().numpy(), axis=1)\n",
    "\n",
    "    if test_dataset:\n",
    "      # all the labels in test_dataset is contradiction\n",
    "      if pred[0] == 2:\n",
    "        matched += 1\n",
    "      # the label of contradiction is -1 in test_dataset\n",
    "      if label != -1:\n",
    "        print('exception in test dataset')\n",
    "    elif pred[0] == label:\n",
    "      matched += 1\n",
    "\n",
    "    if i != 0 and i % 500 == 0:\n",
    "      print(f'Step at: {i / 500}, accu: {matched / N }, matched {matched} out of {i}')\n",
    "\n",
    "  return matched / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_model_evaluation(model, encoded_dataset, mnli_dataset, test_dataset):\n",
    "  eval_start_time = time.time()\n",
    "  acc = evaluate(model, encoded_dataset, mnli_dataset, test_dataset)\n",
    "  eval_end_time = time.time()\n",
    "  eval_duration_time = eval_end_time - eval_start_time\n",
    "  print(\"\\nEND INFO:\")\n",
    "  print(\"Evaluate total time (seconds): {0:.1f}\".format(eval_duration_time))\n",
    "  print(f'Evaluate end accuracy is {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_size_of_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "acc = time_model_evaluation(model, encoded_dataset['validation_matched'], dataset[\"validation_matched\"], test_dataset=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PrunBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_layers_to_keep = ['0', '1', '2', '3', '4', '5']\n",
    "encoder_layers_to_keep = ['0', '1', '2', '4']\n",
    "\n",
    "def prune_state_dict(state_dict):\n",
    "    def create_pruning_pass(layers_to_keep, layer_name):\n",
    "        keep_layers = sorted(\n",
    "            [int(layer_string) for layer_string in layers_to_keep]\n",
    "        )\n",
    "        mapping_dict = {}\n",
    "        for i in range(len(keep_layers)):\n",
    "            mapping_dict[str(keep_layers[i])] = str(i)\n",
    "\n",
    "        regex = re.compile(\"^{layer}.*\\.layers\\.(\\d+)\".format(layer=layer_name))\n",
    "        return {\"substitution_regex\": regex, \"mapping_dict\": mapping_dict}\n",
    "\n",
    "    pruning_passes = []\n",
    "    if encoder_layers_to_keep:\n",
    "        pruning_passes.append(create_pruning_pass(encoder_layers_to_keep, \"encoder\"))\n",
    "\n",
    "    new_state_dict = {}\n",
    "    for layer_name in state_dict.keys():\n",
    "        match = re.search(\"\\.layer\\.(\\d+)\\.\", layer_name)\n",
    "        # if layer has no number in it, it is a supporting layer, such as an\n",
    "        # embedding\n",
    "        if not match:\n",
    "            # print(f'keeps layer name = {layer_name}.')\n",
    "            new_state_dict[layer_name] = state_dict[layer_name]\n",
    "            continue\n",
    "\n",
    "        # otherwise, layer should be pruned.\n",
    "        original_layer_number = match.group(1)\n",
    "\n",
    "        # figure out which mapping dict to replace from\n",
    "        for pruning_pass in pruning_passes:\n",
    "            if original_layer_number in pruning_pass[\"mapping_dict\"]:\n",
    "                new_layer_number = pruning_pass[\"mapping_dict\"][original_layer_number]\n",
    "                idx = layer_name.find(str(original_layer_number))\n",
    "                new_state_key = (\n",
    "                    layer_name[: idx]\n",
    "                    + new_layer_number\n",
    "                    + layer_name[idx + 1 :]\n",
    "                )\n",
    "                # print(f'original layer name = {layer_name}.           , original_layer_number = {original_layer_number}')\n",
    "                # print(f'new layer name      = {new_state_key}         , new_layer_number =  {new_layer_number}')\n",
    "                new_state_dict[new_state_key] = state_dict[layer_name]\n",
    "\n",
    "    return new_state_dict\n",
    "\n",
    "def load_state_dict(state_dict, strict=True):\n",
    "  new_state_dict = prune_state_dict(state_dict)\n",
    "  return new_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_state_dict = load_state_dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels, num_hidden_layers=len(encoder_layers_to_keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained weight for pruned_model\n",
    "pruned_model.load_state_dict(pruned_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train prunBERT\n",
    "prunBERT_trainer = Trainer(\n",
    "    pruned_model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prunBERT_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_size_of_model(pruned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model.to(device)\n",
    "acc = time_model_evaluation(pruned_model, encoded_dataset['validation_matched'], dataset[\"validation_matched\"], test_dataset=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_weights = pruned_model.distilbert.embeddings.word_embeddings.weight.clone()\n",
    "embedding_fp32 = nn.Embedding.from_pretrained(original_weights)\n",
    "qconfig = quantization.float_qparams_weight_only_qconfig\n",
    "embedding_fp32.qconfig = qconfig\n",
    "embedding_quantized = nnq.Embedding.from_float(embedding_fp32)\n",
    "pruned_model.distilbert.embeddings.word_embeddings = embedding_quantized\n",
    "\n",
    "# quantization\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "triple_model = torch.quantization.quantize_dynamic(\n",
    "    pruned_model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_size_of_model(quantized_model)\n",
    "print_size_of_model(triple_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model.to(device)\n",
    "acc = time_model_evaluation(quantized_model, encoded_dataset['validation_matched'], dataset[\"validation_matched\"], test_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triple_model.to(device)\n",
    "acc = time_model_evaluation(triple_model, encoded_dataset['validation_matched'], dataset[\"validation_matched\"], test_dataset=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "501005",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
