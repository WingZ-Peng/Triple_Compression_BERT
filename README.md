# **Triple_Compression_BERT**

> Main focus on fine-tune a pretrained distilled-BERT model for MNLI task. 
>
> By use knowledege distillation, pruning and quantization
>
> We call it ***Triple_Compression BERT project***.

https://arxiv.org/pdf/1910.06188.pdf
https://arxiv.org/pdf/2006.16362.pdf